この記事で取りあつかうこと
* 機械翻訳基礎
* Google Colaboratryの利用方法
* OpenNMT-pyの利用方法

この記事で取りあつかわないこと
* 機械翻訳モデルの詳細
* 機械翻訳の応用例の紹介

## 機械翻訳概要

現在の機械翻訳は対訳コーパスと呼ばれるビッグデータを利用した、機械学習(ディープラーニング)により実装されています。
対訳コーパスは、翻訳元の言語(原言語)と翻訳先の言語(目的言語)の文が対となったデータのことです。
小さいものでも約10万文対、大きいものだと1000文対以上もあります。
現在はEncoderDecoderと呼ばれるディープラーニングモデルを利用するのが主流です。
詳しい説明は省きますが簡単に言うと、EncoderDecoderはある時系列データを別の時系列データに変換するモデルです。
EncoderDecoderをはじめとするディープラーニングモデルを1から実装するのは大変です。
そこで今回はオープンソースであるOpenNMT-pyを利用して実装します。

## 実行環境

機械翻訳をはじめとする、多くのディープラーニングタスクでは計算に膨大がかかり、それゆえGPUの利用が推奨されています。
しかし、すべての人がGPUを用意できるわけではありません。
そこで今回はクラウドサービス、Google Colaboratry(通称Colab)を利用することによりGPUを使用します。
Colabは[こちら](https://colab.research.google.com/notebooks/intro.ipynb?hl=ja)からアクセスできます。
Colabを利用するためにはGoogleアカウントが必要になります。
また今回は対訳コーパスとして[京都関連文書対訳コーパス(KFTT)](http://www.phontron.com/kftt/index-ja.html)を使用します。
KFTTはWikipediaの京都関連の記事を対象とした約44万文対からなる日英対訳コーパスです。
KFTTにはすでに分かち書き済みのテキストがあるため、それを使用します。
今回は学習時間の都合上、日英ともに文長が10以下である文対のみを対象とします。
翻訳モデルは先述の通り、[OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py)を利用します

## 実装
実装の手順を紹介します。
なお、今回紹介するノートブックは[こちら]()で公開しています。
#### notebookを立ち上げる
[Colab](https://colab.research.google.com/notebooks/intro.ipynb?hl=ja)にアクセスし、
```ファイル -> ノートブックを新規作成``` を選択します。
Jupyter Notebook形式のページが表示されるはずです。
#### GPUを利用可能にする
```ランタイム -> ランタイムのタイプを変更```を選択し、「ハードウェアアクセラレータ」をGPUに設定します。
#### GPUの種類を確認する
ノートブックのセルに以下のように記述し、Ctrl + Enterで実行します。

```
# GPUの種類を確認する
!nvidia-smi
```
実行すると以下のようなフォーマットが出力されます。
```
Wed Apr  8 06:09:41 2020       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla P4            Off  | 00000000:00:04.0 Off |                  N/A |
| N/A   50C    P8     8W /  75W |      0MiB /  7611MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
```
Colabでは記事投稿時、4種類の性能が異なるGPUを提供しており、それらはランタイム接続時にランダムで割り当てられます。
今回は「Tesla P4」が割り当てられました。(出力結果中央参照)
筆者の感覚では、「Tesla K80」「Tesla P4」は性能が低く、「Tesla T4」、「Tesla P100」は性能が高いです。
性能の高いGPUは性能の低いGPUよりも約3倍速いと言われています。([参考](https://qiita.com/dasaku_ai/items/56633b4a3a93099be23c))
#### 対訳データを用意する。
続いて以下のセルを実行します。
```
# 京都関連文書対訳コーパス(KFTT)をダウンロードする。
!wget http://www.phontron.com/kftt/download/kftt-data-1.0.tar.gz
# tar.gzファイルを解凍する。
!tar -zxvf /content/kftt-data-1.0.tar.gz
# Googleドライブに分かち書き済みの対訳データをコピーする。
!cp -r kftt-data-1.0/data/tok/ 'drive/My Drive/'
```
上記ではKFTTをダウンロードし、解凍し、分かち書き済みのデータをGoogle Driveに保存します。
実際にうまくいってるか確かめるために以下のコードを実行します。
```
!ls '/content/drive/My Drive'tok
```

うまくいっていれば以下のように出力されるはずです。
```
kyoto-dev.en  kyoto-test.en  kyoto-train.cln.en  kyoto-train.en  kyoto-tune.en
kyoto-dev.ja  kyoto-test.ja  kyoto-train.cln.ja  kyoto-train.ja  kyoto-tune.ja
```
#### 実行準備をする
まず、```%cd drive/My Drive```でカレントディレクトリを変更します。
次に、ボキャブラリおよびモデルを保存するためのディレクトリを作成します。
```
!mkdir data
!mkdir save
```
#### 最大文長を制限する。
最大文長を制限するために以下の関数を作成します。
```
def limiter(data_dir, src_data, tgt_data, n=50):
  '''limiter
  コーパスの最大文長を制限する。
  ----------------------------------------
  引数
  data_dir : コーパスのディレクトリ
  src_data : 原言語のデータ
  tgt_data : 目的言語のデータ
  n : 最大文長
  ----------------------------------------
  '''
  # 書き込み用のデータファイルを作成する。
  with open(f'{data_dir}/limit-{src_data}','w', encoding='utf8') as wsrc:
     with open(f'{data_dir}/limit-{tgt_data}','w', encoding='utf8') as wtgt:
       # 元のデータファイルを読み込む
       with open(f'{data_dir}/{src_data}', encoding='utf8') as rsrc:
         with open(f'{data_dir}/{tgt_data}', encoding='utf8') as rtgt:
           # それぞれ一行ずつ読み込みリストに格納する。
           src_lines = rsrc.read().strip().split('\n')
           tgt_lines = rtgt.read().strip().split('\n')
           for src, tgt in zip(src_lines, tgt_lines):
             # 最大文長がn以下のもののみ書き込む。
             if len(src.split()) <= n and len(tgt.split()) <= n:
               wsrc.write(src + '\n')
               wtgt.write(tgt + '\n')
```
次に以下のセルを実行し、上記の関数を適用します。
```
## 文長制限するファイルの一覧
files = ['train','dev', 'test']
for f in files:
  # 最大文長10とする
  limiter('tok', f'kyoto-{f}.en', f'kyoto-{f}.ja' ,n=10)
```
実行後、もう一度```!ls tok```でディレクトリの確認を行います。
結果以下のようになります。
```
kyoto-dev.en   kyoto-train.cln.en  kyoto-tune.en       limit-kyoto-test.en
kyoto-dev.ja   kyoto-train.cln.ja  kyoto-tune.ja       limit-kyoto-test.ja
kyoto-test.en  kyoto-train.en	   limit-kyoto-dev.en  limit-kyoto-train.en
kyoto-test.ja  kyoto-train.ja	   limit-kyoto-dev.ja  limit-kyoto-train.ja
```
以降、limitから始まるファイルのみを使用します。
#### OpenNMT-pyをインストールする。
セルに以下のように記述し、実行します。
```
!pip install OpenNMT-py
```
これでOpenNMT-pyをインストールできます。
#### 前処理を実行する。




